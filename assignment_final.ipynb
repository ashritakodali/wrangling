{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashritakodali/wrangling/blob/main/assignment_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/DS3001/wrangling"
      ],
      "metadata": {
        "id": "QCfU17fVfc3W",
        "outputId": "87623fb3-b513-4944-911a-9b23e7c62759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QCfU17fVfc3W",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'wrangling'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 83 (delta 31), reused 11 (delta 11), pack-reused 43\u001b[K\n",
            "Receiving objects: 100% (83/83), 10.85 MiB | 22.81 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MJE81uZUfuea"
      },
      "id": "MJE81uZUfuea",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The paper is about the process of data cleaning and how to deal with likely incorrect or missing values. Removing the missing values is a technique that is possible; however, like other data cleaning techniques it should be considered thoroughly before removing key data and other aspects in a dataframe.\n",
        "2. Tidy data standard's goal is to make data cleaning a lot simpler and easier by standardizing it. Essentailly, it is to make sure that it is ease to clean data properly.\n",
        "3. The sentence is trying to analogize how families are different and messy in their own unique way like each dataset it. Each dataset is going to be different and the same techniques applied on one data set will not necessarily work on others. The other sentence is trying to indicate that data is a lot more complicated that it actually is. In essence, it is trying to capture tha fact that data is not as clear cut or easy to understand than it is made out to be.\n",
        "4. Wickham defines values as an object that can be numeric or categorical belonging to a variable and observation. Wickham defines a variable as a collection of values that measure the same attribute. Wickham defines an observation as a collection of values that measure a variable.\n",
        "5. Tidy data is defined as the standard way of mapping the meaning of a dataset to its structure. In a tidy data set, each variable forms a columb, each observation forms a row, each type of observation unit forms a table.\n",
        "6. The 5 most common problems with messy datasets include column headers are counted in as values rather than their variable names, multiple variables are stored in one column, variables are stored in both rows and columns, multiple types of observational units are stored in the same table, A single observational unit is stred in multiple tables. The dataset in table 4 is messy because it has observations, the different income levels, stored as variables. To melt a data set, we stack it or convert columns into rows.\n",
        "7. Table 11 is messy because it has many missing values and has the each of the days of the week as columns rather than a dedicated column called day of the week. Table 12 is tidy because it fixes the issue by making a column to indicate the day of the week.\n",
        "8. The chicken-and-egg problem is essentially the fact that \"if tidy data is only as useful as the tools that work with it, then tidy tools will be inextricably linked to tidy data\". So if the tools are not essentially updated, then tidy data is harder and harder to create and maintain.\n"
      ],
      "metadata": {
        "id": "hK7VgbSUB8b9"
      },
      "id": "hK7VgbSUB8b9"
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing data sets\n",
        "df_airbnb = pd.read_csv('./wrangling/assignment/data/airbnb_hw.csv', low_memory=False)\n",
        "df_sharks = pd.read_csv('./wrangling/assignment/data/sharks.csv', low_memory=False)\n",
        "df = pd.read_parquet('./wrangling/assignment/data/justice_data.parquet')\n",
        "\n",
        "# part 1:\n",
        "print(df_airbnb.shape)\n",
        "df_airbnb.head()\n",
        "price = df_airbnb['Price']\n",
        "price_new = price.str.replace(\",\", \"\")\n",
        "price_new = pd.to_numeric(price, errors = \"coerce\")\n",
        "\n",
        "sum_boolean = []\n",
        "for each_observation in price:\n",
        "  boolean = each_observation.isnumeric()\n",
        "  sum_boolean.append(boolean)\n",
        "\n",
        "sum_boolean_new = []\n",
        "for each_observation in price_new:\n",
        "  boolean = isinstance(each_observation, float)\n",
        "  sum_boolean_new.append(boolean)\n",
        "\n",
        "print(\"This is how many values are numeric in the original price column:\",\n",
        "      sum(sum_boolean))\n",
        "print(\"This is how many values are numeric in the new price column:\",\n",
        "      sum(sum_boolean_new))\n",
        "\n",
        "print(price.unique())\n",
        "\n",
        "print(price_new.unique())\n",
        "\n",
        "df_airbnb[\"Price\"] = price_new\n",
        "\n",
        "# part 2:\n",
        "print(df_sharks[\"Type\"].value_counts())\n",
        "type1 = df_sharks[\"Type\"]\n",
        "type1 = type1.replace([\"Invalid\", \"Questionable\", \"Unconfirmed\", \"Unverified\",\n",
        "                       \"Under investigation\"], \"Unknown\")\n",
        "type1 = type1.replace(['Sea Disaster', 'Boat','Boating','Boatomg', \"Watercraft\"],\n",
        "                      'Water-Related')\n",
        "print(type1.value_counts())\n",
        "\n",
        "df_sharks[\"Type\"] = type1\n",
        "\n",
        "# part 3:\n",
        "release_column = df['WhetherDefendantWasReleasedPretrial']\n",
        "print(release_column.value_counts())\n",
        "print(release_column.unique())\n",
        "release = release_column.replace(9,np.nan)\n",
        "print(sum(release_column.isnull()))\n",
        "print(release_column.value_counts())\n",
        "df['WhetherDefendantWasReleasedPretrial'] = release\n",
        "\n",
        "# Part 4:\n",
        "length = df['ImposedSentenceAllChargeInContactEvent']\n",
        "type2 = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
        "\n",
        "length = pd.to_numeric(length,errors='coerce')\n",
        "length_NA = length.isnull()\n",
        "print( np.sum(length_NA))\n",
        "\n",
        "print( pd.crosstab(length_NA, type2))\n",
        "\n",
        "length = length.mask( type2 == 4, 0)\n",
        "length = length.mask( type2 == 9, np.nan)\n",
        "\n",
        "length_NA = length.isnull()\n",
        "print( pd.crosstab(length_NA, type2))\n",
        "print( np.sum(length_NA))\n",
        "\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = length\n"
      ],
      "metadata": {
        "id": "0XaMOJtIflJW",
        "outputId": "20a5ebe2-6612-4534-92ba-ef8060861114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0XaMOJtIflJW",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30478, 13)\n",
            "This is how many values are numeric in the original price column: 30297\n",
            "This is how many values are numeric in the new price column: 30478\n",
            "['145' '37' '28' '199' '549' '149' '250' '90' '270' '290' '170' '59' '49'\n",
            " '68' '285' '75' '100' '150' '700' '125' '175' '40' '89' '95' '99' '499'\n",
            " '120' '79' '110' '180' '143' '230' '350' '135' '85' '60' '70' '55' '44'\n",
            " '200' '165' '115' '74' '84' '129' '50' '185' '80' '190' '140' '45' '65'\n",
            " '225' '600' '109' '1,990' '73' '240' '72' '105' '155' '160' '42' '132'\n",
            " '117' '295' '280' '159' '107' '69' '239' '220' '399' '130' '375' '585'\n",
            " '275' '139' '260' '35' '133' '300' '289' '179' '98' '195' '29' '27' '39'\n",
            " '249' '192' '142' '169' '1,000' '131' '138' '113' '122' '329' '101' '475'\n",
            " '238' '272' '308' '126' '235' '315' '248' '128' '56' '207' '450' '215'\n",
            " '210' '385' '445' '136' '247' '118' '77' '76' '92' '198' '205' '299'\n",
            " '222' '245' '104' '153' '349' '114' '320' '292' '226' '420' '500' '325'\n",
            " '307' '78' '265' '108' '123' '189' '32' '58' '86' '219' '800' '335' '63'\n",
            " '229' '425' '67' '87' '1,200' '158' '650' '234' '310' '695' '400' '166'\n",
            " '119' '62' '168' '340' '479' '43' '395' '144' '52' '47' '529' '187' '209'\n",
            " '233' '82' '269' '163' '172' '305' '156' '550' '435' '137' '124' '48'\n",
            " '279' '330' '5,000' '134' '378' '97' '277' '64' '193' '147' '186' '264'\n",
            " '30' '3,000' '112' '94' '379' '57' '415' '236' '410' '214' '88' '66' '71'\n",
            " '171' '157' '545' '1,500' '83' '96' '1,800' '81' '188' '380' '255' '505'\n",
            " '54' '33' '174' '93' '740' '640' '1,300' '440' '599' '357' '1,239' '495'\n",
            " '127' '5,999' '178' '348' '152' '242' '183' '253' '750' '259' '365' '273'\n",
            " '197' '397' '103' '389' '355' '559' '38' '203' '999' '141' '162' '333'\n",
            " '698' '46' '360' '895' '10' '41' '206' '281' '449' '388' '212' '102'\n",
            " '201' '2,750' '4,750' '432' '675' '167' '390' '298' '339' '194' '302'\n",
            " '211' '595' '191' '53' '361' '480' '8,000' '4,500' '459' '997' '345'\n",
            " '216' '218' '111' '735' '276' '91' '490' '850' '398' '36' '775' '267'\n",
            " '625' '336' '2,500' '176' '725' '3,750' '469' '106' '460' '287' '575'\n",
            " '227' '263' '25' '228' '208' '177' '880' '148' '116' '685' '470' '217'\n",
            " '164' '61' '645' '699' '405' '252' '319' '268' '419' '343' '525' '311'\n",
            " '840' '154' '294' '950' '409' '184' '257' '204' '241' '2,000' '412' '121'\n",
            " '288' '196' '900' '647' '524' '1,750' '309' '510' '1,495' '1,700' '799'\n",
            " '383' '372' '492' '327' '1,999' '656' '224' '173' '875' '1,170' '795'\n",
            " '690' '146' '465' '1,100' '151' '274' '429' '825' '282' '256' '1,111'\n",
            " '620' '271' '161' '51' '855' '579' '1,174' '430' '20' '899' '649' '485'\n",
            " '181' '455' '4,000' '243' '342' '590' '560' '374' '437' '232' '359' '985'\n",
            " '31' '244' '254' '723' '237' '428' '370' '34' '1,400' '580' '2,520' '221'\n",
            " '749' '1,600' '2,695' '306' '202' '680' '570' '520' '223' '2,295' '213'\n",
            " '1,065' '346' '24' '286' '296' '266' '26' '995' '1,368' '393' '182' '635'\n",
            " '258' '780' '589' '347' '1,250' '1,350' '446' '3,200' '1,050' '1,650'\n",
            " '1,550' '975' '323' '6,500' '2,499' '1,850' '2,250' '715' '461' '540'\n",
            " '356' '439' '384' '569' '1,900' '22' '785' '626' '830' '318' '444' '321'\n",
            " '401' '1,499' '888' '369' '770' '386' '366' '344' '630' '313' '597' '262'\n",
            " '509' '10,000' '278' '312' '789' '1,195' '422' '21' '765' '3,500' '945'\n",
            " '326' '3,100' '2,486' '3,390' '1,356' '2,599' '472' '454' '328' '396'\n",
            " '291']\n",
            "[145.  37.  28. 199. 549. 149. 250.  90. 270. 290. 170.  59.  49.  68.\n",
            " 285.  75. 100. 150. 700. 125. 175.  40.  89.  95.  99. 499. 120.  79.\n",
            " 110. 180. 143. 230. 350. 135.  85.  60.  70.  55.  44. 200. 165. 115.\n",
            "  74.  84. 129.  50. 185.  80. 190. 140.  45.  65. 225. 600. 109.  nan\n",
            "  73. 240.  72. 105. 155. 160.  42. 132. 117. 295. 280. 159. 107.  69.\n",
            " 239. 220. 399. 130. 375. 585. 275. 139. 260.  35. 133. 300. 289. 179.\n",
            "  98. 195.  29.  27.  39. 249. 192. 142. 169. 131. 138. 113. 122. 329.\n",
            " 101. 475. 238. 272. 308. 126. 235. 315. 248. 128.  56. 207. 450. 215.\n",
            " 210. 385. 445. 136. 247. 118.  77.  76.  92. 198. 205. 299. 222. 245.\n",
            " 104. 153. 349. 114. 320. 292. 226. 420. 500. 325. 307.  78. 265. 108.\n",
            " 123. 189.  32.  58.  86. 219. 800. 335.  63. 229. 425.  67.  87. 158.\n",
            " 650. 234. 310. 695. 400. 166. 119.  62. 168. 340. 479.  43. 395. 144.\n",
            "  52.  47. 529. 187. 209. 233.  82. 269. 163. 172. 305. 156. 550. 435.\n",
            " 137. 124.  48. 279. 330. 134. 378.  97. 277.  64. 193. 147. 186. 264.\n",
            "  30. 112.  94. 379.  57. 415. 236. 410. 214.  88.  66.  71. 171. 157.\n",
            " 545.  83.  96.  81. 188. 380. 255. 505.  54.  33. 174.  93. 740. 640.\n",
            " 440. 599. 357. 495. 127. 178. 348. 152. 242. 183. 253. 750. 259. 365.\n",
            " 273. 197. 397. 103. 389. 355. 559.  38. 203. 999. 141. 162. 333. 698.\n",
            "  46. 360. 895.  10.  41. 206. 281. 449. 388. 212. 102. 201. 432. 675.\n",
            " 167. 390. 298. 339. 194. 302. 211. 595. 191.  53. 361. 480. 459. 997.\n",
            " 345. 216. 218. 111. 735. 276.  91. 490. 850. 398.  36. 775. 267. 625.\n",
            " 336. 176. 725. 469. 106. 460. 287. 575. 227. 263.  25. 228. 208. 177.\n",
            " 880. 148. 116. 685. 470. 217. 164.  61. 645. 699. 405. 252. 319. 268.\n",
            " 419. 343. 525. 311. 840. 154. 294. 950. 409. 184. 257. 204. 241. 412.\n",
            " 121. 288. 196. 900. 647. 524. 309. 510. 799. 383. 372. 492. 327. 656.\n",
            " 224. 173. 875. 795. 690. 146. 465. 151. 274. 429. 825. 282. 256. 620.\n",
            " 271. 161.  51. 855. 579. 430.  20. 899. 649. 485. 181. 455. 243. 342.\n",
            " 590. 560. 374. 437. 232. 359. 985.  31. 244. 254. 723. 237. 428. 370.\n",
            "  34. 580. 221. 749. 306. 202. 680. 570. 520. 223. 213. 346.  24. 286.\n",
            " 296. 266.  26. 995. 393. 182. 635. 258. 780. 589. 347. 446. 975. 323.\n",
            " 715. 461. 540. 356. 439. 384. 569.  22. 785. 626. 830. 318. 444. 321.\n",
            " 401. 888. 369. 770. 386. 366. 344. 630. 313. 597. 262. 509. 278. 312.\n",
            " 789. 422.  21. 765. 945. 326. 472. 454. 328. 396. 291.]\n",
            "Unprovoked             4716\n",
            "Provoked                593\n",
            "Invalid                 552\n",
            "Sea Disaster            239\n",
            "Watercraft              142\n",
            "Boat                    109\n",
            "Boating                  92\n",
            "Questionable             10\n",
            "Unconfirmed               1\n",
            "Unverified                1\n",
            "Under investigation       1\n",
            "Boatomg                   1\n",
            "Name: Type, dtype: int64\n",
            "Unprovoked       4716\n",
            "Provoked          593\n",
            "Water-Related     583\n",
            "Unknown           565\n",
            "Name: Type, dtype: int64\n",
            "1    19154\n",
            "0     3801\n",
            "9       31\n",
            "Name: WhetherDefendantWasReleasedPretrial, dtype: int64\n",
            "[9 0 1]\n",
            "0\n",
            "1    19154\n",
            "0     3801\n",
            "9       31\n",
            "Name: WhetherDefendantWasReleasedPretrial, dtype: int64\n",
            "9053\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914     0    0\n",
            "True                                                 0     0    0  8779  274\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914  8779    0\n",
            "True                                                 0     0    0     0  274\n",
            "274 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In problem 1, I decided to first look at the type of data that is stored in the column and saw that it was being stored as a character because of how it read in values that were greater than 999. I first decided to fix this issue by forcing the column to become a numeric vector instead. In problem 2, I first decided to see the different levels of the type variable and saw that several of the levels essentially meant the same thing. I made any observation that resembled the meaning of an \"n/a\" or \"unknown\" observation as \"Unknown\" and any water related reason as \"Water-Related\". In problem 3, the codebook says that the value 9 is unclear and I decided to replace that with na's. In the end, I saw there are 31 missing values. In problem 4, I decided to force the column as a numeric values and then I decided to replace anything that had a length of 0 with the number 4 from the type column. Finally, I replaced anything that had na with the number 9. I saw that there are 274 missing values."
      ],
      "metadata": {
        "id": "7mf0oOohHiSv"
      },
      "id": "7mf0oOohHiSv"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}